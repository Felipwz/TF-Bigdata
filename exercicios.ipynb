{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe29031",
   "metadata": {},
   "source": [
    "# Exercícios de Big Data com Spark + Iceberg + Hive Metastore + HDFS\n",
    "\n",
    "Este notebook contém 20 exercícios desenvolvidos para o ambiente lab (Jupyter + Spark + Iceberg + HDFS + Hive Metastore).\n",
    "\n",
    "**Instruções:**\n",
    "- Execute as células em ordem sequencial\n",
    "- Alguns exercícios dependem dos anteriores\n",
    "- Verifique os outputs de cada célula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f140a",
   "metadata": {},
   "source": [
    "## Exercício 1: Criar um DataFrame simples\n",
    "Crie um DataFrame com três linhas e duas colunas (`id`, `nome`) e mostre seu conteúdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc71865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar Spark Session\n",
    "spark = SparkSession.builder.appName(\"Exercicio1\").getOrCreate()\n",
    "\n",
    "# Criar DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Carlos\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"nome\"])\n",
    "\n",
    "# Mostrar conteúdo\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398eb481",
   "metadata": {},
   "source": [
    "## Exercício 2: Salvar DataFrame no HDFS como CSV\n",
    "Crie um DataFrame e salve em `hdfs://namenode:9000/data/ex1.csv` no formato CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar no HDFS como CSV\n",
    "df.write.mode(\"overwrite\").csv(\"hdfs://namenode:9000/data/ex1.csv\", header=True)\n",
    "\n",
    "print(\"DataFrame salvo em HDFS com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad423f25",
   "metadata": {},
   "source": [
    "## Exercício 3: Ler CSV do HDFS\n",
    "Leia o arquivo salvo no exercício anterior usando `spark.read.csv()` e exiba o DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2098e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler CSV do HDFS\n",
    "df_read = spark.read.csv(\"hdfs://namenode:9000/data/ex1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Exibir DataFrame\n",
    "df_read.show()\n",
    "df_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12434fec",
   "metadata": {},
   "source": [
    "## Exercício 4: Criar namespace Iceberg\n",
    "Crie um namespace chamado `lab.db` no catálogo Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be684d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar namespace\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lab.db\")\n",
    "\n",
    "print(\"Namespace lab.db criado com sucesso!\")\n",
    "\n",
    "# Verificar namespaces existentes\n",
    "spark.sql(\"SHOW NAMESPACES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2031d76",
   "metadata": {},
   "source": [
    "## Exercício 5: Criar tabela Iceberg\n",
    "Crie uma tabela Iceberg `lab.db.pessoas (id INT, nome STRING)` usando SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0188c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabela Iceberg\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lab.db.pessoas (\n",
    "        id INT,\n",
    "        nome STRING\n",
    "    ) USING ICEBERG\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tabela lab.db.pessoas criada com sucesso!\")\n",
    "\n",
    "# Verificar tabelas no namespace\n",
    "spark.sql(\"SHOW TABLES IN lab.db\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efb4a5",
   "metadata": {},
   "source": [
    "## Exercício 6: Inserir dados na tabela Iceberg\n",
    "Insira 3 valores manualmente usando SQL `INSERT INTO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir dados na tabela\n",
    "spark.sql(\"INSERT INTO lab.db.pessoas VALUES (1, 'Alice')\")\n",
    "spark.sql(\"INSERT INTO lab.db.pessoas VALUES (2, 'Bob')\")\n",
    "spark.sql(\"INSERT INTO lab.db.pessoas VALUES (3, 'Carlos')\")\n",
    "\n",
    "print(\"Dados inseridos com sucesso!\")\n",
    "\n",
    "# Verificar dados inseridos\n",
    "spark.sql(\"SELECT * FROM lab.db.pessoas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972815a5",
   "metadata": {},
   "source": [
    "## Exercício 7: Ler tabela Iceberg\n",
    "Faça uma query `SELECT * FROM lab.db.pessoas` e exiba o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5113287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler tabela Iceberg\n",
    "df_pessoas = spark.sql(\"SELECT * FROM lab.db.pessoas\")\n",
    "\n",
    "# Exibir resultado\n",
    "df_pessoas.show()\n",
    "\n",
    "print(f\"Total de colunas: {len(df_pessoas.columns)}\")\n",
    "print(f\"Colunas: {df_pessoas.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7616e7c",
   "metadata": {},
   "source": [
    "## Exercício 8: Contar registros\n",
    "Conte quantos registros existem na tabela `lab.db.pessoas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar registros\n",
    "count = spark.sql(\"SELECT COUNT(*) as total FROM lab.db.pessoas\")\n",
    "count.show()\n",
    "\n",
    "# Alternativa usando DataFrame API\n",
    "df_pessoas = spark.table(\"lab.db.pessoas\")\n",
    "total = df_pessoas.count()\n",
    "print(f\"Total de registros: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00598520",
   "metadata": {},
   "source": [
    "## Exercício 9: Atualizar um registro\n",
    "Atualize um nome usando Iceberg SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar dados antes da atualização\n",
    "print(\"Dados antes da atualização:\")\n",
    "spark.sql(\"SELECT * FROM lab.db.pessoas\").show()\n",
    "\n",
    "# Atualizar registro\n",
    "spark.sql(\"UPDATE lab.db.pessoas SET nome = 'Alice Silva' WHERE nome = 'Alice'\")\n",
    "\n",
    "# Mostrar dados após atualização\n",
    "print(\"Dados após atualização:\")\n",
    "spark.sql(\"SELECT * FROM lab.db.pessoas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fb913",
   "metadata": {},
   "source": [
    "## Exercício 10: Deletar um registro\n",
    "Remova uma linha da tabela usando `DELETE FROM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b375202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar dados antes da deleção\n",
    "print(\"Dados antes da deleção:\")\n",
    "spark.sql(\"SELECT * FROM lab.db.pessoas\").show()\n",
    "\n",
    "# Deletar registro\n",
    "spark.sql(\"DELETE FROM lab.db.pessoas WHERE id = 3\")\n",
    "\n",
    "# Mostrar dados após deleção\n",
    "print(\"Dados após deleção:\")\n",
    "spark.sql(\"SELECT * FROM lab.db.pessoas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395133b",
   "metadata": {},
   "source": [
    "## Exercício 11: Criar tabela particionada\n",
    "Crie uma tabela Iceberg com partição por ano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabela particionada\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lab.db.vendas (\n",
    "        id INT,\n",
    "        valor DOUBLE,\n",
    "        ano INT\n",
    "    ) USING ICEBERG\n",
    "    PARTITIONED BY (ano)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tabela lab.db.vendas criada com sucesso!\")\n",
    "\n",
    "# Verificar detalhes da tabela\n",
    "spark.sql(\"DESCRIBE EXTENDED lab.db.vendas\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6c621",
   "metadata": {},
   "source": [
    "## Exercício 12: Inserir dados particionados\n",
    "Insira dados variando o valor de `ano`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9944f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir dados de diferentes anos\n",
    "spark.sql(\"INSERT INTO lab.db.vendas VALUES (1, 100.50, 2022)\")\n",
    "spark.sql(\"INSERT INTO lab.db.vendas VALUES (2, 250.75, 2022)\")\n",
    "spark.sql(\"INSERT INTO lab.db.vendas VALUES (3, 150.00, 2023)\")\n",
    "spark.sql(\"INSERT INTO lab.db.vendas VALUES (4, 300.25, 2023)\")\n",
    "spark.sql(\"INSERT INTO lab.db.vendas VALUES (5, 450.80, 2024)\")\n",
    "\n",
    "print(\"Dados inseridos com sucesso!\")\n",
    "\n",
    "# Verificar dados inseridos\n",
    "spark.sql(\"SELECT * FROM lab.db.vendas ORDER BY ano, id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26d9bb",
   "metadata": {},
   "source": [
    "## Exercício 13: Consultar apenas um particionamento\n",
    "Leia somente as vendas do ano 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e629aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultar apenas vendas de 2023\n",
    "print(\"Vendas do ano 2023:\")\n",
    "spark.sql(\"SELECT * FROM lab.db.vendas WHERE ano = 2023\").show()\n",
    "\n",
    "# Alternativa usando DataFrame API\n",
    "df_vendas = spark.table(\"lab.db.vendas\")\n",
    "vendas_2023 = df_vendas.filter(df_vendas.ano == 2023)\n",
    "print(\"\\nUsando DataFrame API:\")\n",
    "vendas_2023.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512854a9",
   "metadata": {},
   "source": [
    "## Exercício 14: Ver metadados da tabela\n",
    "Use `DESCRIBE HISTORY` e `DESCRIBE DETAIL` para ver metadados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver histórico da tabela\n",
    "print(\"Histórico da tabela vendas:\")\n",
    "spark.sql(\"DESCRIBE HISTORY lab.db.vendas\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Ver detalhes da tabela\n",
    "print(\"Detalhes da tabela vendas:\")\n",
    "spark.sql(\"DESCRIBE DETAIL lab.db.vendas\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7772af3a",
   "metadata": {},
   "source": [
    "## Exercício 15: Criar tabela Iceberg a partir de DataFrame\n",
    "Crie um DataFrame artificial e grave diretamente com `writeTo()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame artificial\n",
    "data = [\n",
    "    (1, \"Produto A\", 50.00),\n",
    "    (2, \"Produto B\", 75.50),\n",
    "    (3, \"Produto C\", 120.00),\n",
    "    (4, \"Produto D\", 35.99),\n",
    "    (5, \"Produto E\", 200.00)\n",
    "]\n",
    "\n",
    "df_produtos = spark.createDataFrame(data, [\"id\", \"produto\", \"preco\"])\n",
    "\n",
    "print(\"DataFrame criado:\")\n",
    "df_produtos.show()\n",
    "\n",
    "# Gravar como tabela Iceberg\n",
    "df_produtos.writeTo(\"lab.db.tabela_df\").createOrReplace()\n",
    "\n",
    "print(\"\\nTabela criada com sucesso!\")\n",
    "\n",
    "# Verificar tabela criada\n",
    "spark.sql(\"SELECT * FROM lab.db.tabela_df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56466da6",
   "metadata": {},
   "source": [
    "## Exercício 16: Converter tabela para Iceberg\n",
    "Crie uma tabela Parquet simples e converta para Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e513f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame\n",
    "data = [\n",
    "    (1, \"Item A\"),\n",
    "    (2, \"Item B\"),\n",
    "    (3, \"Item C\")\n",
    "]\n",
    "\n",
    "df_items = spark.createDataFrame(data, [\"id\", \"descricao\"])\n",
    "\n",
    "# Salvar como tabela Parquet\n",
    "df_items.write.mode(\"overwrite\").saveAsTable(\"lab.db.tabela_parquet\", format=\"parquet\")\n",
    "\n",
    "print(\"Tabela Parquet criada!\")\n",
    "\n",
    "# Ler dados da tabela Parquet e recriar como Iceberg\n",
    "df_parquet = spark.table(\"lab.db.tabela_parquet\")\n",
    "df_parquet.writeTo(\"lab.db.tabela_iceberg_convertida\").createOrReplace()\n",
    "\n",
    "print(\"Tabela convertida para Iceberg!\")\n",
    "\n",
    "# Verificar nova tabela\n",
    "spark.sql(\"SELECT * FROM lab.db.tabela_iceberg_convertida\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d59ae73",
   "metadata": {},
   "source": [
    "## Exercício 17: Leitura incremental (Time Travel)\n",
    "Volte para uma versão anterior da tabela usando `VERSION AS OF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d0f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar histórico da tabela\n",
    "print(\"Histórico da tabela vendas:\")\n",
    "spark.sql(\"DESCRIBE HISTORY lab.db.vendas\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Ler versão atual\n",
    "print(\"Dados da versão atual:\")\n",
    "spark.sql(\"SELECT * FROM lab.db.vendas\").show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Ler versão específica (versão 1) - Time Travel\n",
    "print(\"Dados da versão 1 (Time Travel):\")\n",
    "spark.sql(\"SELECT * FROM lab.db.vendas VERSION AS OF 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00878035",
   "metadata": {},
   "source": [
    "## Exercício 18: Exportar tabela Iceberg para CSV\n",
    "Leia a tabela Iceberg e salve para `hdfs://namenode:9000/export/vendas.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler tabela Iceberg\n",
    "df_export = spark.sql(\"SELECT * FROM lab.db.vendas\")\n",
    "\n",
    "print(\"Dados da tabela vendas:\")\n",
    "df_export.show()\n",
    "\n",
    "# Exportar para CSV no HDFS\n",
    "output_path = \"hdfs://namenode:9000/export/vendas.csv\"\n",
    "df_export.write.mode(\"overwrite\").csv(output_path, header=True)\n",
    "\n",
    "print(f\"\\nTabela exportada para {output_path} com sucesso!\")\n",
    "\n",
    "# Verificar se o arquivo foi criado\n",
    "print(\"\\nLendo arquivo exportado:\")\n",
    "df_exported = spark.read.csv(output_path, header=True, inferSchema=True)\n",
    "df_exported.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a7b01",
   "metadata": {},
   "source": [
    "## Exercício 19: Criar um Dashboard no Superset\n",
    "\n",
    "### Objetivo\n",
    "Adicionar o banco Trino no Superset, conectar ao catálogo Iceberg e criar uma visualização simples.\n",
    "\n",
    "### Passos:\n",
    "\n",
    "#### 1. Acessar o Superset\n",
    "- Acesse: `http://localhost:8088`\n",
    "- Faça login com as credenciais configuradas\n",
    "\n",
    "#### 2. Adicionar conexão com Trino\n",
    "1. Vá em **Data** → **Databases** → **+ Database**\n",
    "2. Selecione **Trino**\n",
    "3. Connection string: `trino://trino@trino:8080/iceberg`\n",
    "4. Teste e salve\n",
    "\n",
    "#### 3. Conectar ao catálogo Iceberg\n",
    "1. **Data** → **Datasets** → **+ Dataset**\n",
    "2. Database: Trino, Schema: lab.db, Table: vendas\n",
    "3. Salve o dataset\n",
    "\n",
    "#### 4. Criar visualização\n",
    "1. **Charts** → **+ Chart**\n",
    "2. Configure métricas e dimensões\n",
    "3. Execute e salve\n",
    "\n",
    "#### 5. Criar Dashboard\n",
    "1. **Dashboards** → **+ Dashboard**\n",
    "2. Adicione os charts\n",
    "3. Organize e salve\n",
    "\n",
    "**Nota:** Este exercício é manual e deve ser realizado na interface web do Superset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3456edc",
   "metadata": {},
   "source": [
    "## Exercício 20: Ler tabela Iceberg via Trino\n",
    "\n",
    "### Queries SQL para executar no Superset ou CLI do Trino:\n",
    "\n",
    "```sql\n",
    "-- Conectar ao catálogo\n",
    "USE iceberg;\n",
    "\n",
    "-- Listar schemas\n",
    "SHOW SCHEMAS;\n",
    "\n",
    "-- Ler tabela pessoas\n",
    "SELECT * FROM iceberg.lab.db.pessoas;\n",
    "\n",
    "-- Ler tabela vendas com agregação\n",
    "SELECT \n",
    "    ano,\n",
    "    COUNT(*) as total_vendas,\n",
    "    SUM(valor) as valor_total,\n",
    "    AVG(valor) as valor_medio\n",
    "FROM iceberg.lab.db.vendas\n",
    "GROUP BY ano\n",
    "ORDER BY ano;\n",
    "```\n",
    "\n",
    "**Nota:** Execute essas queries na interface do Superset ou no CLI do Trino."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
